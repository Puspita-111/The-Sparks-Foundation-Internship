# -*- coding: utf-8 -*-
"""supervisedlearning sparks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_4N3CWFNDgjRHFdrGlzzZPjIJALfUqRg

# Name- Puspita Saha

# Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.It infers a function from labeled training data consisting of a set of training examples. 
#In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias).

# Importing Libraries
"""

!pip install lazypredict
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import lazypredict
from lazypredict.Supervised import LazyRegressor

"""# Importing the Dataset"""

dataset = pd.read_csv('student_scores - student_scores.csv')

dataset.isnull().sum()

x = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 1].values

print(x)

print(y)

"""# Splitting the Dataset into Train and Test Set"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.1, random_state=1)

print(x_train)

print(x_test)

print(y_test)

print(y_train)

"""# Plotting the Distribution of scores"""

dataset.plot(x='Hours', y='Scores', style='o')  
plt.title('Hours vs Scores')  
plt.xlabel('Hours Studied')  
plt.ylabel('Score achived')  
plt.show()

clf = LazyRegressor(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(x_train, x_test, y_train, y_test)
models

"""# LassoLars is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients.
# Lasso model fit with Least Angle Regression a.k.a. Lars It is a Linear Model trained with an L1 prior as regularizer.                                      The optimization objective for Lasso is:                                      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1

# Training the LassoLars model on the whole dataset
"""

from sklearn import linear_model
regressor = linear_model.LassoLars(alpha=0.01)
x_train= x_train.reshape(-1,1)
regressor.fit(x_train, y_train, None)

x_test= x_test.reshape(-1,1)
y_pred= regressor.predict(x_test)

regressor.score(x_train, y_train, sample_weight=None)

"""# Visualizing the LassoLars model result"""

plt.scatter(x, y, color = 'red')
plt.plot(x, regressor.predict(x), color = 'blue')
plt.title('Hours vs Scores')
plt.xlabel('Study Hours')
plt.ylabel('Score Achived')
plt.show()

"""#Comparing Predicting the Actual vs Predicted values"""

df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})  
df

"""# Predicted Score if a student Studies 9.25 hours a day."""

regressor.predict([[9.25]])

"""# Mean Absolute Error"""

from sklearn import metrics  
print('Mean Absolute Error:', 
      metrics.mean_absolute_error(y_test, y_pred))